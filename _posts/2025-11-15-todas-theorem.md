---
layout: post
title: toda's theorem
subtitle: PH is a subset of P^#SAT
tags: [complexity theory]
type: r
---
Today, I read Toda's 1989 paper. Titled "PP is as hard as the polynomial-time hierarchy", Toda proved that $$PH \subseteq P^{\#SAT}$$. This is quite the strange theorem since we know that $$P^{SAT} = {\Delta}_{2}^{P}$$, which is the set of languages contained within $$PH$$ at the second level. $$\#SAT$$ and $$SAT$$ are closely related, so why does one give so much more computational power than the othe (or in a different perspective -- why is alternation weaker than we had believed!)? Maybe one insight is that instances of $$\#SAT$$ could have *exponentially many* satisfying models (to which the $$\#SAT$$ oracle can know right away). 

However, this is still strange to me. For one, $$M_{SAT}$$ still needs to comb through an exponential number of models in the worst case to find a solution (unless $P=NP$), so it still encodes some exponential amount of computation. Hence, the fact that $$P^{SAT}$$ is contained in $$PH$$ but $$P^{\#SAT}$$ is not suggests that there's something weirder going on here. In my opinion, the key intuition here is that there is a "double" barrier against heuristics that machines computing $$\#SAT$$ cannot exploit. The decider for $$SAT$$ could get lucky and find a model for a formula quickly, but the same doesn't apply for $$\#SAT$$. Deciders for $$\#SAT$$ can get lucky in finding one model, but would still need to count the (potentially exponential in the worst case) number of satisfying models to the formula. This might also suggest that $$NP$$ can be characterized as the set of languages which have a high variance of problem difficulty (and thus heuristics can work well) whereas $$\#P$$ can be characterized as the set of languages which have strict problem difficulty (and thus heuristics tend to fail) [^1].

[^1]: **A slightly related leeway**: This past semester, I took a class on SAT-solving, and there were two interesting phenomena discussed about the hardness of boolean formulas: the first is that SAT-solvers work remarkably well in practice because most formulas don't tend to be that difficult to solve + heuristics work pretty well. The second is that there exists a ratio of clauses to variables in random 3-SAT formulas which are exceptionally difficulty (that being ~4.2). Interestingly, a lot of work in modern complexity theory has been in average-case hardness, hardness of approximation, randomness in computation, etc etc. I.e., it covers a lot of work in computational hardness of heuristic programs. It will be interesting to see where this study of randomness & heuristics will take the field in the future! If you're interested in works of this nature, check out this [connection between statistical physics and complexity theory](https://arxiv.org/pdf/1405.3558)!

Hence, a program which takes advantage of being able to encode some exponential amount of computation might just be powerful enough to compute languages more powerful than those at arbitrary levels of the polynomial hierarcy (which can only support a constant number of alternations). This also creates an interesting point of view for computational resources: if something encodes "exponential" of some resource, then there's probably a hacky way to use its oracle to do some crazy things, like making P with a $$\#SAT$$ oracle as hard as $$PH$$!

### References
>[Toda's 1989 Paper](https://dl.acm.org/doi/10.1137/0220053)
